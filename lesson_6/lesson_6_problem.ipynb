{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-LAB LESSON 6: Deep Reinforcement Learning\n",
    "\n",
    "In this lesson we will use the CartPole environment and we will see how to create and work with a neural network using Kears on top of Tensorflow.\n",
    "\n",
    "## CartPole\n",
    "The environment used is **CartPole** (taken from the book of Sutton and Barto as visible in the figure)\n",
    "\n",
    "![Cartpole](images/cartpole.jpg)\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, tensorflow.keras, random, numpy\n",
    "module_path = os.path.abspath(os.path.join('../tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **state** of environment is represented as a tuple of 4 values: \n",
    "- *Cart Position* range from -4.9 to 4.8\n",
    "- *Cart Velocity* range from -inf to +inf\n",
    "- *Pole Angle* range from -24 deg to 24 deg\n",
    "- *Pole Velocity* range from -inf to +inf\n",
    "\n",
    "The **actions** allowed in the environment are 2:\n",
    "- *action 0*: push cart to left\n",
    "- *action 1*: push cart to right\n",
    "\n",
    "The **reward** is 1 for every step taken, including the termination step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state = env.reset()\n",
    "print(\"STARTING STATE: {}\".format(state))\n",
    "print(\"\\tCart Position: {}\\n\\tCart Velocity {}\\n\\tPole Angle {} \\n\\tPole Velocity {}\".format(state[0], state[1], state[2], state[3]))\n",
    "\n",
    "print(\"\\nPOSSIBLE ACTIONS: \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we still have the standard functionalities of a Gym environment:\n",
    "- step(action): the agent performs action from the current state. Returns a tuple (new_state, reward, done, info) where:\n",
    "    - new_state: is the new state reached as a consequence of the agent's last action\n",
    "    - reward: the reward obtained by the agent in this step\n",
    "    - done: True if the episode is terminal, False otherwise\n",
    "    - info: not used, you can safely discard it\n",
    "\n",
    "- reset(): the environment is reset and the agent goes back to the starting position. Returns the initial state id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with Kears\n",
    "**Keras** is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, R, Theano, or PlaidML. Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible.\n",
    "\n",
    "![Network](images/neural_networks.png)\n",
    "\n",
    "With kears you can easly create a neural network with the **Sequential** module. Before training a neural netowrk you must compile it, selecting the loss function and the optimizer, in our experiment we will use the *mean_squared_error* for the loss function and the *adam* optimizer, that is a standard configuration for a DQN problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = 3\n",
    "layer_size = 5\n",
    "output_layer = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(layer_size, input_dim=input_layer, activation=\"relu\")) #input layer + hidden layer #1\n",
    "model.add(Dense(layer_size, activation=\"relu\")) #hidden layer #2\n",
    "model.add(Dense(layer_size, activation=\"relu\")) #hidden layer #3\n",
    "model.add(Dense(layer_size, activation=\"relu\")) #hidden layer #4\n",
    "model.add(Dense(layer_size, activation=\"relu\")) #hidden layer #5\n",
    "model.add(Dense(output_layer, activation=\"linear\")) #output layer\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='adam') #loss function and optimzer definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras you can compute the output of a network with the **predict** function, that requires in input the values of the input layer nodes and returns the corresponding values of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_network = [random.uniform(0, 1), random.uniform(0, 1), random.uniform(0, 1)]\n",
    "output_network = model.predict(np.array([input_network]))\n",
    "print(\"Input network: {}\".format(input_network))\n",
    "print(\"network Prediction: {}\".format(output_network[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a network in Keras we must use the function **fit**, that take as input:\n",
    "- *input*: the input of the network that we are interested to train\n",
    "- *expected_output*: the output that we consider correct\n",
    "- *epochs*: the number of iteration for the backpropagation (in DQN this value is always 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_network = [random.uniform(0, 1), random.uniform(0, 1), random.uniform(0, 1)]\n",
    "expected_output = [0, 0]\n",
    "\n",
    "print(\"Prediction 'before' training:\")\n",
    "print(model.predict(np.array([input_network])))\n",
    "\n",
    "model.fit(np.array([input_network]), np.array([expected_output]), epochs=1000, verbose=0)\n",
    "\n",
    "print(\"\\nPrediction 'after' training:\")\n",
    "print(model.predict(np.array([input_network])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, remember that for all the methods (*fit*, *predict*, ...) keras requires as input a numpy array of array, for example you must convert your state in the correct **shape**.  Kears will return, in the same way, an array of array, so to extract the corresponding ouutput layer you must select the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.array([0, 0, 0])\n",
    "# model.predict(input_network) will give you a shape error\n",
    "state = state.reshape(1, 3)\n",
    "print(\"Prediction:\", model.predict(state)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Q-Learning\n",
    "\n",
    "Your first assignement is to implement all the functions nexessary for a deep q-learning algorithm. In particular you must implement the following functions: *create_model*, *train_model* and *DQN*.\n",
    "\n",
    "#### Hint:\n",
    "For the experience replay buffer you can use the python data structure *dequeue*, defining the maximum length allowed. With the *random.sample(replay_buffer, size)* function you can sample *size* element from the queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=10000)\n",
    "for _ in range(100): replay_buffer.append(random.uniform(0, 1))\n",
    "    \n",
    "samples = random.sample(replay_buffer, 3) \n",
    "print(\"Get 3 element from replay_buffer:\", samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_size, output_size, hidden_layer_size, hidden_layer_number):\n",
    "    \"\"\"\n",
    "    Create the neural network model with the given parameters\n",
    "    \n",
    "    Args:\n",
    "        input_size: the number of nodes for the input layer\n",
    "        output_size: the number of nodes for the output layer\n",
    "        hidden_layer_size: the number of nodes for each hidden layer\n",
    "        hidden_layer_number: the number of hidden layers\n",
    "        \n",
    "    Returns:\n",
    "        model: the corresponding neural network\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer iniziale di input + hidden layer\n",
    "    model.add(Dense(hidden_layer_size, input_dim=input_size, activation=\"relu\"))\n",
    "    \n",
    "    # Hidden layers intermedi\n",
    "    for _ in range(1, hidden_layer_number):\n",
    "        model.add(Dense(hidden_layer_size, activation=\"relu\"))\n",
    "\n",
    "    # Layer finale di output\n",
    "    model.add(Dense(output_size, activation=\"linear\"))\n",
    "\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_replay(neural_network, memory, batch_size, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        neural_network: the neural network model to train\n",
    "        memory: the memory array on wich perform the training\n",
    "        batch_size: the size of the batch sampled from the memory\n",
    "        gamma: gamma value, the discount factor for the Bellman equation\n",
    "    \"\"\"    \n",
    "    \n",
    "    TRAINING_EPOCHS  = 1\n",
    "    TRAINING_VERBOSE = 0\n",
    "\n",
    "    # Se non abbiamo abbastanza eventi per riempire un mini-batch\n",
    "    # evitiamo di fare training\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    # Ottiene mini-batch dall'esperienza\n",
    "    experience = random.sample(memory, batch_size)\n",
    "    for event in experience:\n",
    "        current_state, action, next_state, reward, done = event\n",
    "        \n",
    "        # Dato uno stato passato genera la nuova risposta del NN\n",
    "        current_state = current_state.reshape(1, 4)\n",
    "        TD_target = neural_network.predict(current_state)[0]\n",
    "\n",
    "        # Calcola reward dell'azione a seconda che lo stato sia terminale o no\n",
    "        if done == True:\n",
    "            TD_target[action] = reward\n",
    "        else:\n",
    "            next_state = next_state.reshape(1, 4)\n",
    "            max_q = max(neural_network.predict(next_state)[0])\n",
    "            TD_target[action] = reward + gamma * max_q\n",
    "\n",
    "        # Training sulle esperienze\n",
    "        TD_target = TD_target.reshape(1, 4)\n",
    "        neural_network.fit(current_state, TD_target, \n",
    "            epochs=TRAINING_EPOCHS, verbose=TRAINING_VERBOSE)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN(environment, neural_network, trials, goal_score, batch_size, epsilon_decay=0.9995):\n",
    "    \"\"\"\n",
    "    Performs the Q-Learning algorithm for a specific environment on a specific neural network model\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        neural_network: the neural network to train\n",
    "        trials: the number of iterations for the training phase\n",
    "        goal_score: the minimum score to consider 'solved' the problem\n",
    "        batch_size: the size of the batch sampled from the memory\n",
    "        epsilon_decay: the dacay value of epsilon for the eps-greedy exploration\n",
    "        \n",
    "    Returns:\n",
    "        score_queue: 1-d dimensional array of the reward obtained at each trial step\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    epsilon = 1.0; epsilon_min = 0.01\n",
    "    score_queue = []\n",
    "\n",
    "    EXPERIENCE_BUFFER_LEN = 10000\n",
    "    experience_buffer = deque(maxlen = EXPERIENCE_BUFFER_LEN)\n",
    "\n",
    "    for trial in range(trials):\n",
    "        \n",
    "        current_state = environment.reset()\n",
    "        current_state = state.reshape(1, 4)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        # Numero di azioni prese per esplorazione o sfruttamento\n",
    "        exploration  = 0\n",
    "        exploitation = 0\n",
    "\n",
    "        # Episodio di addestramento\n",
    "        trial_score = 0\n",
    "        while not done:\n",
    "\n",
    "            # Output del NN\n",
    "            nn_actions_score = neural_network.predict(current_state)[0]\n",
    "                        \n",
    "            # Con probabilità epsilon sceglie di esplorare e con prob. (1 - epsilon) di seguire la policy\n",
    "            if random.uniform(0.0, 1.0) > epsilon:\n",
    "                action = numpy.argmax(nn_actions_score)\n",
    "                exploitation += 1\n",
    "            else:\n",
    "                action = numpy.argmin(nn_actions_score)\n",
    "                exploration += 1\n",
    "\n",
    "            # Rendiamo sempre meno probabile l'esplorazione\n",
    "            epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "            # Esegue l'azione scelta e salva il risultato nella cache di esperienza\n",
    "            next_state, reward, done, _ = environment.step(action)\n",
    "            experience_buffer.append([state, action, next_state, reward, done])\n",
    "            \n",
    "            neural_network = experience_replay(neural_network, experience_buffer, batch_size)\n",
    "            trial_score = trial_score + reward\n",
    "            current_state = next_state\n",
    "            \n",
    "        score_queue.append(trial_score)\n",
    "        if trial_score > goal_score or score_queue == []: \n",
    "            break\n",
    "        \n",
    "        print(\"Episode: {:7.0f}, Score: {:3.0f}, ExpBufferSize: {}, Exploitation: {}, Exploration: {}, EPS: {:3.5f}\"\n",
    "            .format(trial, score_queue[-1], len(experience_buffer), str(exploitation), str(exploration), epsilon))\n",
    "    \n",
    "    return neural_network, score_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "neural_network = create_model(4, 2, 32, 2)\n",
    "neural_network, score = DQN(env, neural_network, trials=1000, goal_score=130, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "The following code executes the DQN and plots the reward function, the execution could require up to 10 minutes on some computer. A more efficent version of the code can be found [here](https://github.com/d-corsi/BasicRL).\n",
    "Correct results for comparison can be found here below. Notice that since the executions are stochastic the charts could differ: the important thing is the global trend and the final convergence to a visible reward improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewser = []\n",
    "window = 10\n",
    "\n",
    "score = rolling(np.array(score), window)\n",
    "rewser.append({\"x\": np.arange(1, len(score) + 1), \"y\": score, \"ls\": \"-\", \"label\": \"DQN\"})\n",
    "plot(rewser, \"Rewards\", \"Episodes\", \"Rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard DQN on CartPole results:**\n",
    "<img src=\"images/results-dqn.png\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
