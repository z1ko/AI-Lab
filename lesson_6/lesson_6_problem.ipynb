{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-LAB LESSON 6: Deep Reinforcement Learning\n",
    "\n",
    "In this lesson we will use the CartPole environment and we will see how to create and work with a neural network using Kears on top of Tensorflow.\n",
    "\n",
    "## CartPole\n",
    "The environment used is **CartPole** (taken from the book of Sutton and Barto as visible in the figure)\n",
    "\n",
    "![Cartpole](images/cartpole.jpg)\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, tensorflow.keras, random, numpy\n",
    "module_path = os.path.abspath(os.path.join('../tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **state** of environment is represented as a tuple of 4 values: \n",
    "- *Cart Position* range from -4.9 to 4.8\n",
    "- *Cart Velocity* range from -inf to +inf\n",
    "- *Pole Angle* range from -24 deg to 24 deg\n",
    "- *Pole Velocity* range from -inf to +inf\n",
    "\n",
    "The **actions** allowed in the environment are 2:\n",
    "- *action 0*: push cart to left\n",
    "- *action 1*: push cart to right\n",
    "\n",
    "The **reward** is 1 for every step taken, including the termination step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING STATE: [-0.00579938 -0.0007288   0.03674978 -0.0240394 ]\n",
      "\tCart Position: -0.005799378004022436\n",
      "\tCart Velocity -0.000728801695898916\n",
      "\tPole Angle 0.036749777798396985 \n",
      "\tPole Velocity -0.024039403923742753\n",
      "\n",
      "POSSIBLE ACTIONS:  2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state = env.reset()\n",
    "print(\"STARTING STATE: {}\".format(state))\n",
    "print(\"\\tCart Position: {}\\n\\tCart Velocity {}\\n\\tPole Angle {} \\n\\tPole Velocity {}\".format(state[0], state[1], state[2], state[3]))\n",
    "\n",
    "print(\"\\nPOSSIBLE ACTIONS: \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we still have the standard functionalities of a Gym environment:\n",
    "- step(action): the agent performs action from the current state. Returns a tuple (new_state, reward, done, info) where:\n",
    "    - new_state: is the new state reached as a consequence of the agent's last action\n",
    "    - reward: the reward obtained by the agent in this step\n",
    "    - done: True if the episode is terminal, False otherwise\n",
    "    - info: not used, you can safely discard it\n",
    "\n",
    "- reset(): the environment is reset and the agent goes back to the starting position. Returns the initial state id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with Kears\n",
    "**Keras** is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, R, Theano, or PlaidML. Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible.\n",
    "\n",
    "![Network](images/neural_networks.png)\n",
    "\n",
    "With kears you can easly create a neural network with the **Sequential** module. Before training a neural netowrk you must compile it, selecting the loss function and the optimizer, in our experiment we will use the *mean_squared_error* for the loss function and the *adam* optimizer, that is a standard configuration for a DQN problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 15:33:26.583376: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-02-09 15:33:26.589014: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-09 15:33:26.600182: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "input_layer = 3\n",
    "layer_size = 5\n",
    "output_layer = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(layer_size, input_dim=input_layer, activation=\"relu\")) #input layer + hidden layer #1\n",
    "model.add(Dense(layer_size, activation=\"relu\")) #hidden layer #2\n",
    "model.add(Dense(layer_size, activation=\"relu\")) #hidden layer #3\n",
    "model.add(Dense(layer_size, activation=\"relu\")) #hidden layer #4\n",
    "model.add(Dense(layer_size, activation=\"relu\")) #hidden layer #5\n",
    "model.add(Dense(output_layer, activation=\"linear\")) #output layer\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='adam') #loss function and optimzer definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras you can compute the output of a network with the **predict** function, that requires in input the values of the input layer nodes and returns the corresponding values of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 15:33:30.321125: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-02-09 15:33:30.379390: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2899885000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input network: [0.10617476863789388, 0.028838591879704834, 0.9263239751335113]\n",
      "network Prediction: [-0.02210776 -0.03628682]\n"
     ]
    }
   ],
   "source": [
    "input_network = [random.uniform(0, 1), random.uniform(0, 1), random.uniform(0, 1)]\n",
    "output_network = model.predict(np.array([input_network]))\n",
    "print(\"Input network: {}\".format(input_network))\n",
    "print(\"network Prediction: {}\".format(output_network[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a network in Keras we must use the function **fit**, that take as input:\n",
    "- *input*: the input of the network that we are interested to train\n",
    "- *expected_output*: the output that we consider correct\n",
    "- *epochs*: the number of iteration for the backpropagation (in DQN this value is always 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 'before' training:\n",
      "[[-0.01937051 -0.03179402]]\n",
      "\n",
      "Prediction 'after' training:\n",
      "[[0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "input_network = [random.uniform(0, 1), random.uniform(0, 1), random.uniform(0, 1)]\n",
    "expected_output = [0, 0]\n",
    "\n",
    "print(\"Prediction 'before' training:\")\n",
    "print(model.predict(np.array([input_network])))\n",
    "\n",
    "model.fit(np.array([input_network]), np.array([expected_output]), epochs=1000, verbose=0)\n",
    "\n",
    "print(\"\\nPrediction 'after' training:\")\n",
    "print(model.predict(np.array([input_network])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, remember that for all the methods (*fit*, *predict*, ...) keras requires as input a numpy array of array, for example you must convert your state in the correct **shape**.  Kears will return, in the same way, an array of array, so to extract the corresponding ouutput layer you must select the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.0122903  0.00565043]\n"
     ]
    }
   ],
   "source": [
    "state = np.array([0, 0, 0])\n",
    "# model.predict(input_network) will give you a shape error\n",
    "state = state.reshape(1, 3)\n",
    "print(\"Prediction:\", model.predict(state)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Q-Learning\n",
    "\n",
    "Your first assignement is to implement all the functions nexessary for a deep q-learning algorithm. In particular you must implement the following functions: *create_model*, *train_model* and *DQN*.\n",
    "\n",
    "#### Hint:\n",
    "For the experience replay buffer you can use the python data structure *dequeue*, defining the maximum length allowed. With the *random.sample(replay_buffer, size)* function you can sample *size* element from the queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get 3 element from replay_buffer: [0.11087038738747923, 0.06471072930711519, 0.24706944929712527]\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = deque(maxlen=10000)\n",
    "for _ in range(100): replay_buffer.append(random.uniform(0, 1))\n",
    "    \n",
    "samples = random.sample(replay_buffer, 3) \n",
    "print(\"Get 3 element from replay_buffer:\", samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_size, output_size, hidden_layer_size, hidden_layer_number):\n",
    "    \"\"\"\n",
    "    Create the neural network model with the given parameters\n",
    "    \n",
    "    Args:\n",
    "        input_size: the number of nodes for the input layer\n",
    "        output_size: the number of nodes for the output layer\n",
    "        hidden_layer_size: the number of nodes for each hidden layer\n",
    "        hidden_layer_number: the number of hidden layers\n",
    "        \n",
    "    Returns:\n",
    "        model: the corresponding neural network\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer iniziale di input + hidden layer\n",
    "    model.add(Dense(hidden_layer_size, input_dim=input_size, activation=\"relu\"))\n",
    "    \n",
    "    # Hidden layers intermedi\n",
    "    for _ in range(1, hidden_layer_number):\n",
    "        model.add(Dense(hidden_layer_size, activation=\"relu\"))\n",
    "\n",
    "    # Layer finale di output\n",
    "    model.add(Dense(output_size, activation=\"linear\"))\n",
    "\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_replay(neural_network, memory, batch_size, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        neural_network: the neural network model to train\n",
    "        memory: the memory array on wich perform the training\n",
    "        batch_size: the size of the batch sampled from the memory\n",
    "        gamma: gamma value, the discount factor for the Bellman equation\n",
    "    \"\"\"    \n",
    "    \n",
    "    TRAINING_EPOCHS  = 1\n",
    "    TRAINING_VERBOSE = 0\n",
    "\n",
    "    # Se non abbiamo abbastanza eventi per riempire un mini-batch\n",
    "    # evitiamo di fare training\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    # Ottiene mini-batch dall'esperienza\n",
    "    experience = random.sample(memory, batch_size)\n",
    "    for event in experience:\n",
    "        current_state, action, next_state, reward, done = event\n",
    "        \n",
    "        # Dato uno stato passato genera la nuova risposta del NN\n",
    "        nn_actions_score = neural_network.predict(np.array([current_state]))[0]\n",
    "        action = numpy.argmax(nn_actions_score)\n",
    "\n",
    "        # Calcola reward dell'azione a seconda che lo stato sia terminale o no\n",
    "        if done == True:\n",
    "            nn_actions_score[action] = reward\n",
    "        else:\n",
    "            max_q = max(neural_network.predict(np.array([next_state]))[0])\n",
    "            nn_actions_score[action] = reward + max_q * gamma\n",
    "\n",
    "        # Training sulle esperienze\n",
    "        target_actions_score = np.array([nn_actions_score])\n",
    "        neural_network.fit(np.array([current_state]), target_actions_score, \n",
    "            epochs=TRAINING_EPOCHS, verbose=TRAINING_VERBOSE)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN(environment, neural_network, trials, goal_score, batch_size, epsilon_decay=0.9995):\n",
    "    \"\"\"\n",
    "    Performs the Q-Learning algorithm for a specific environment on a specific neural network model\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        neural_network: the neural network to train\n",
    "        trials: the number of iterations for the training phase\n",
    "        goal_score: the minimum score to consider 'solved' the problem\n",
    "        batch_size: the size of the batch sampled from the memory\n",
    "        epsilon_decay: the dacay value of epsilon for the eps-greedy exploration\n",
    "        \n",
    "    Returns:\n",
    "        score_queue: 1-d dimensional array of the reward obtained at each trial step\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    epsilon = 1.0; epsilon_min = 0.01\n",
    "    score = 0; score_queue = []\n",
    "\n",
    "    EXPERIENCE_BUFFER_LEN = 10000\n",
    "    experience_buffer = deque(maxlen = EXPERIENCE_BUFFER_LEN)\n",
    "\n",
    "    trial_step  = 0\n",
    "    for trial in range(trials):\n",
    "        \n",
    "        state   = environment.reset()\n",
    "        done    = False\n",
    "        epsilon = 1.0\n",
    "\n",
    "        # Numero di azioni prese per esplorazione o sfruttamento\n",
    "        exploration  = 0\n",
    "        exploitation = 0\n",
    "\n",
    "        # Episodio di addestramento\n",
    "        trial_score = 0\n",
    "        while not done:\n",
    "\n",
    "            # Output del NN\n",
    "            nn_actions_score = neural_network.predict(np.array([state]))[0]\n",
    "                        \n",
    "            # Con probabilitÃ  epsilon sceglie di esplorare e con prob. (1 - epsilon) di sfruttare\n",
    "            if random.uniform(0.0, 1.0) > epsilon:\n",
    "                action = numpy.argmax(nn_actions_score)\n",
    "                exploitation += 1\n",
    "            else:\n",
    "                action = numpy.argmin(nn_actions_score)\n",
    "                exploration += 1\n",
    "\n",
    "            # Rendiamo sempre meno probabile l'esplorazione\n",
    "            epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "            # Esegue l'azione scelta e salva il risultato nella cache di esperienza\n",
    "            next_state, reward, done, _ = environment.step(action)\n",
    "            experience_buffer.append([state, action, next_state, reward, done])\n",
    "            \n",
    "            trial_score += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Addestra il NN ogni 4 step\n",
    "            trial_step += 1\n",
    "            if trial_step % 4 == 0:\n",
    "                experience_replay(neural_network, experience_buffer, batch_size)\n",
    "            \n",
    "\n",
    "        score_queue.append(trial_score)\n",
    "        if score > goal_score or score_queue == []: \n",
    "            break\n",
    "        \n",
    "        print(\"Episode: {:7.0f}, Score: {:3.0f}, ExpBufferSize: {}, Exploitation: {}, Exploration: {}, EPS: {:3.5f}\"\n",
    "            .format(trial, score_queue[-1], len(experience_buffer), str(exploitation), str(exploration), epsilon))\n",
    "    \n",
    "    return neural_network, score_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:       0, Score:   9, ExpBufferSize: 9, Exploitation: 0, Exploration: 9, EPS: 0.99551\n",
      "Episode:       1, Score:  11, ExpBufferSize: 20, Exploitation: 0, Exploration: 11, EPS: 0.99451\n",
      "Episode:       2, Score:  10, ExpBufferSize: 30, Exploitation: 0, Exploration: 10, EPS: 0.99501\n",
      "Episode:       3, Score:   9, ExpBufferSize: 39, Exploitation: 0, Exploration: 9, EPS: 0.99551\n",
      "Episode:       4, Score:   8, ExpBufferSize: 47, Exploitation: 0, Exploration: 8, EPS: 0.99601\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5326/3912573185.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CartPole-v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mneural_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mneural_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5326/2978460539.py\u001b[0m in \u001b[0;36mDQN\u001b[0;34m(environment, neural_network, trials, goal_score, batch_size, epsilon_decay)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mtrial_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrial_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mexperience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperience_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5326/3822267712.py\u001b[0m in \u001b[0;36mexperience_replay\u001b[0;34m(neural_network, memory, batch_size, gamma)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Dato uno stato passato genera la nuova risposta del NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mnn_actions_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_actions_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1623\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2970\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 2972\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   2973\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "neural_network = create_model(4, 2, 32, 2)\n",
    "neural_network, score = DQN(env, neural_network, trials=1000, goal_score=130, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "The following code executes the DQN and plots the reward function, the execution could require up to 10 minutes on some computer. A more efficent version of the code can be found [here](https://github.com/d-corsi/BasicRL).\n",
    "Correct results for comparison can be found here below. Notice that since the executions are stochastic the charts could differ: the important thing is the global trend and the final convergence to a visible reward improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5326/2918896720.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mrewser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"DQN\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Rewards\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Episodes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Rewards\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "rewser = []\n",
    "window = 10\n",
    "\n",
    "score = rolling(np.array(score), window)\n",
    "rewser.append({\"x\": np.arange(1, len(score) + 1), \"y\": score, \"ls\": \"-\", \"label\": \"DQN\"})\n",
    "plot(rewser, \"Rewards\", \"Episodes\", \"Rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard DQN on CartPole results:**\n",
    "<img src=\"images/results-dqn.png\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
